{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exceptional-burns",
   "metadata": {},
   "source": [
    "This preprocessing file cleans up the survey and reference data files and creates reference files in the resources folder for the analysis carried out in the 2020 descriptive notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hybrid-warning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import datetime as dt\n",
    "import math\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "# set useful variables for accessing the files\n",
    "\n",
    "here = os.getcwd()\n",
    "flora_h = F\"{here}/resources/reference-data/\"\n",
    "flora_h_ws = F\"{flora_h}atlasws/\"\n",
    "flora_h_55 = F\"{flora_h}atlas5x5/\"\n",
    "data_2020 = F\"{here}/resources/survey-data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dimensional-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data files\n",
    "\n",
    "# start by organizing them into dictionaries.\n",
    "\n",
    "my_data_methods = {\"csv\":pd.read_csv}\n",
    "\n",
    "# dict for the 2020 survey data files\n",
    "d_files = {\n",
    "    \"surveys\":\"surveys.csv\",\n",
    "    \"map_keys\":\"map-keys.csv\",    \n",
    "}\n",
    "\n",
    "# dict for the reference files that are not Welten Sutter or Flora Helvetica 5X5 lists\n",
    "\n",
    "w_lists = {\n",
    "    \"list_2014\":\"BL_WL_2014_modified.csv\",\n",
    "    \"under_sampled\":\"taxa_sous_echantillonnes.csv\",\n",
    "    \"red_list\":\"redlist2019.csv\",\n",
    "    \"cert_list\": \"Certification_specieslist_2021.csv\"\n",
    "}\n",
    "\n",
    "# dict for the Welten-Sutter map reference files, downloaded from here: https://www.infoflora.ch/de/daten/artenliste-welten-sutter.html\n",
    "# all observations included in this report were conducted within one of these geographic boundaries\n",
    "\n",
    "ws_lists ={\n",
    "    \"151\":\"AtlasWS_151_Biel.csv\",\n",
    "    \"252\":\"AtlasWS_252_Erlach.csv\",\n",
    "    \"300\":\"AtlasWS_300_Aarberg.csv\",\n",
    "    \"301\":\"AtlasWS_301_Bueren.csv\",\n",
    "    \"154\":\"AtlasWS_154_Grenchen.csv\",\n",
    "    \"572\":\"AtlasWS_572_Beatenberg.csv\",\n",
    "    \"573\":\"AtlasWS_573_Interlaken.csv\",\n",
    "    \"226\":\"AtlasWS_226_Estavayer.csv\",\n",
    "    \"251\":\"AtlasWS_251_BernWest.csv\",\n",
    "    \"145\":\"AtlasWS_145_LesRangiers.csv\"\n",
    "}\n",
    "\n",
    "\n",
    "# dict for the Flora Helvetica 5x5 map reference files, downloaded from here: https://www.infoflora.ch/de/daten/artenliste-5x5-km.html\n",
    "# all observations included in this report were conducted within one of these geographic boundaries\n",
    "\n",
    "# housekeeping: 585220 is separated by \",\" not \";\" like the rest of the data sources\n",
    "df = pd.read_csv(\"resources/reference-data/atlas5x5/Atlas5x5_585_220.csv\", sep = \",\", encoding=\"utf-16\")\n",
    "df.to_csv('resources/reference-data/atlas5x5/Atlas5x5_585_220_1.csv', sep=';', encoding = \"utf-16\", index = False)\n",
    "\n",
    "fx_lists = {\n",
    "    \"585215\":\"Atlas5x5_585_215.csv\", # Ipsach, Bielersee\n",
    "    \"585220\":\"Atlas5x5_585_220_1.csv\", # Biel Stadt, Suze / Bielersee\n",
    "    \"580220\":\"Atlas5x5_580_220.csv\", # Biel Mett, Suze\n",
    "    \"580215\":\"Atlas5x5_580_215.csv\", # Port, Nidau-Bueren Kanal\n",
    "    \"625165\":\"Atlas5x5_625_165.csv\", # Untersee, Thunersee\n",
    "    \"625170\":\"Atlas5x5_625_170.csv\", # Sundlauenen, Thunersee\n",
    "    \"550185\":\"Atlas5x5_550_185.csv\", # Estavayer, Lac de Neuchatel\n",
    "    \"575210\":\"Atlas5x5_575_210.csv\", # Leuecherz, Bielersee\n",
    "    \"600200\":\"Atlas5x5_600_200.csv\", # Bern west, Aare\n",
    "    \"575245\":\"Atlas5x5_575_245.csv\",  # Saint-Ursanne, Aare\n",
    "    \"545180\":\"Atlas5x5_545_180.csv\",\n",
    "    \"575215\":\"Atlas5x5_575_215.csv\"\n",
    "}\n",
    "\n",
    "# convenience method to gather up all the files:\n",
    "\n",
    "def get_the_data(file_exts, a_dir, methods, this_method=\"csv\", myencoding=None):\n",
    "    wiw = {}\n",
    "    for k,v in file_exts.items():\n",
    "        if myencoding == None:\n",
    "            wiw.update({k:methods[this_method](F\"{a_dir}{v}\")})            \n",
    "        else:\n",
    "            wiw.update({k:methods[this_method](F\"{a_dir}{v}\",sep = \";\", encoding=myencoding)})\n",
    "    return wiw\n",
    "\n",
    "# use the get_the_data method to collect these files\n",
    "\n",
    "data_and_keys = get_the_data(d_files, data_2020, my_data_methods, this_method=\"csv\")\n",
    "watch_lists = get_the_data(w_lists, flora_h, my_data_methods, this_method=\"csv\")\n",
    "welt_sut =  get_the_data(ws_lists, flora_h_ws, my_data_methods, this_method=\"csv\", myencoding = \"utf-16\" )\n",
    "fivex =  get_the_data(fx_lists, flora_h_55, my_data_methods, this_method=\"csv\", myencoding = \"utf-16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-advance",
   "metadata": {},
   "source": [
    "Organization of the reference files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "north-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "# housekeeping\n",
    "\n",
    "# housekeeping: make sure that each data set has the column \"species\", with the value species:\n",
    "watch_lists[\"list_2014\"][\"species\"] = watch_lists[\"list_2014\"].Latin\n",
    "watch_lists[\"under_sampled\"][\"species\"] = watch_lists[\"under_sampled\"].taxon\n",
    "watch_lists[\"red_list\"][\"species\"] = watch_lists[\"red_list\"].scientific_name\n",
    "watch_lists[\"cert_list\"][\"species\"] = watch_lists[\"cert_list\"][\"Short Name\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "vanilla-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a species slug (genus-species) to link data from across the survey and reference files. \n",
    "# This is necessary as some species columns have only \"Genus species\", some include subspecies, and some include the taxonomic reference.\n",
    "\n",
    "# function to make the species slugs\n",
    "def to_species_slug(x):\n",
    "    try: \n",
    "        int_data = x.split()\n",
    "        data = int_data[:2]\n",
    "        data = \"-\".join(data)\n",
    "        data = data.lower()\n",
    "    except:\n",
    "        data = \"none\"\n",
    "    return data\n",
    "\n",
    "# create a new column to hold the slug\n",
    "\n",
    "for element in [fivex, welt_sut, watch_lists]:\n",
    "    for the_data in element:\n",
    "        element[the_data]['species_slug'] = 'none'\n",
    "\n",
    "# make the species slug for all reference files\n",
    "\n",
    "for element in [fivex, welt_sut, watch_lists]:\n",
    "    for the_data in element:\n",
    "        element[the_data]['species_slug'] = element[the_data].species.map(lambda x: to_species_slug(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brief-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add identifying columns to the reference datasets\n",
    "\n",
    "# add a column to identify the map source for the geographic data:\n",
    "\n",
    "for element in [fivex, welt_sut]:\n",
    "    for the_data in element:\n",
    "        element[the_data]['map'] = the_data\n",
    "        element[the_data]['spec_map'] = list(zip(element[the_data].species_slug,element[the_data].map))\n",
    "        \n",
    "# add a column to identify watch list:\n",
    "\n",
    "for element in [watch_lists]:\n",
    "    for the_data in element:\n",
    "        if the_data == \"list_2014\":\n",
    "            element[the_data]['watch_list'] = element[the_data][the_data]\n",
    "        else:\n",
    "            element[the_data]['watch_list'] = the_data\n",
    "\n",
    "# housekeeping: fill in nan values in the watchlist and certification list reference files.\n",
    "\n",
    "fill_nans = watch_lists[\"list_2014\"].copy()\n",
    "fill_nans = fill_nans.fillna(0)\n",
    "watch_lists.update({\"list_2014\":fill_nans[fill_nans.watch_list != 0]})\n",
    "\n",
    "fill_nans = watch_lists[\"cert_list\"].copy()\n",
    "fill_nans = fill_nans.fillna(0)\n",
    "watch_lists.update({\"cert_list\":fill_nans[fill_nans.watch_list != 0]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-pasta",
   "metadata": {},
   "source": [
    "### Species name and observations: harmonizing taxonomy\n",
    "\n",
    "The genus-species nomenclature will be used to group observations.\n",
    "\n",
    "All observations will be classified according to that standard. As a result subspecies will be folded in with the parent species. This is a reflection of the survey method and the expectation of reasonable results, not a prioritization of importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accessible-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def account_for_subspecies(an_array, a_dict):\n",
    "    for element in an_array:\n",
    "        try:\n",
    "            a_dict[element[0]].append(element[1])\n",
    "        except:\n",
    "            a_dict[element[0]] = [element[1]]\n",
    "    return a_dict\n",
    "a_dict ={}\n",
    "\n",
    "for element in [fivex, welt_sut, watch_lists]:\n",
    "    for label in element:\n",
    "        # use this data frame\n",
    "        som_data = element[label].copy()\n",
    "        \n",
    "        # group by species slug and count the number of unique species values\n",
    "        c_s_p_s = som_data.groupby('species_slug', as_index=False).species.nunique()        \n",
    "        \n",
    "        # just the records with more than one species value\n",
    "        m_t_one = c_s_p_s[c_s_p_s.species > 1].species_slug\n",
    "        \n",
    "        # pair the species_slug to the species name:\n",
    "        mto = som_data.loc[som_data.species_slug.isin(m_t_one)][['species_slug', 'species']].copy().to_numpy()\n",
    "        \n",
    "        # update the dict\n",
    "        account_for_subspecies(mto, a_dict)\n",
    "\n",
    "# the species_slugs that account for more than one sub species\n",
    "sub_species_accounted =  {k:list(set(v)) for k,v in a_dict.items()}\n",
    "\n",
    "# the species_slugs\n",
    "gs_parent = list(sub_species_accounted.keys())\n",
    "\n",
    "# the number of species_slugs\n",
    "number_of_gs = len(gs_parent)\n",
    "\n",
    "# the the number of sub species accounted for\n",
    "number_of_ss = sum([len(v) for k,v in sub_species_accounted.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-momentum",
   "metadata": {},
   "source": [
    "### Collect welten sutter and 5x5 observations and export to a json dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "popular-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all the observations into one df\n",
    "fx = pd.concat([v[['species_slug', 'map', 'spec_map']] for k,v in fivex.items()])\n",
    "wsx = pd.concat([v[['species_slug', 'map', 'spec_map']] for k,v in welt_sut.items()])\n",
    "f_w_obs = pd.concat([fx, wsx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "better-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather up the map names\n",
    "m_ap_columns = f_w_obs.map.unique()\n",
    "\n",
    "# create a column for each map, indicate\n",
    "for col in m_ap_columns:    \n",
    "    f_w_obs[col] = f_w_obs['map'] == col\n",
    "\n",
    "obs_map =f_w_obs.groupby(['species_slug']).sum()\n",
    "\n",
    "obs_map['maps'] = obs_map.index.map(lambda x: f_w_obs[f_w_obs.species_slug == x]['map'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lovely-throat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Which maps 'trifolium-incarnatum' were found in? indifferent of subspecies?:\n",
      "\n",
      "['585215', '585220', '580215', '600200', '545180', '151', '252', '300', '154', '226', '251', '145']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#exports the dictionary to a .json file\n",
    "nt = obs_map['maps'].to_dict()\n",
    "\n",
    "ntx = {k:list(v) for k,v in nt.items()}\n",
    "\n",
    "with open(F\"{here}/output/ws_list.json\",\"w\") as afile:\n",
    "    json.dump(ntx,afile)\n",
    "    \n",
    "print(F\"\\nWhich maps 'trifolium-incarnatum' were found in? indifferent of subspecies?:\\n\\n{ntx['trifolium-incarnatum']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "incident-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use red_list in w_lists to identify the status of species encountered.\n",
    "# use the status in the \"CH\" column for \"n\"\n",
    "# use the below dictionary to translate the status\n",
    "\n",
    "# combine this with watch list / black list data from list_2014 in watch_lists and export to .json dictionary.\n",
    "# not this\n",
    "\n",
    "endangerdict = {\n",
    "    \"LC\": \"Least Concern\",\n",
    "    \"NT\": \"Near Threatened\",\n",
    "    \"VU\": \"Vulnerable\",\n",
    "    \"EN\": \"Endangered\",\n",
    "    \"CR\": \"Critically Endangered\"\n",
    "}\n",
    "\n",
    "# map this to\n",
    "# df.this_key.map(lamda x: region_dict[x])\n",
    "\n",
    "regiondict = {\n",
    "    \"JU\": \"Jura\",\n",
    "    \"MP\": \"Central Plateau\",\n",
    "    \"CH\": \"National\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-advocate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "latest-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format scientific name in the sample dataset\n",
    "\n",
    "samples = data_and_keys['surveys'].copy()\n",
    "\n",
    "# dictionary of replacement values that are incorrect\n",
    "\n",
    "replacedict = {\n",
    "    'verbanum bonariensis ':'verbena bonariensis',\n",
    "    'medicago varia':'medicago sativa',\n",
    "    \"oenothera\":\"oenothera biennis\",\n",
    "    \"geranium pratens\":\"geranium pratense\",\n",
    "    \"senecio jacobaea\": \"jacobaea vulgaris\",\n",
    "    \"oenothera biennis \": \"oenothera biennis\",\n",
    "    \"oenothera biennis agg.\": \"oenothera biennis\",\n",
    "    \"solidalgo canadensis\": \"solidago canadensis\",\n",
    "    \"verbascum lynchitis\":\"verbascum lychnitis\",\n",
    "    \"verbascum negris\":\"verbascum nigrum\",\n",
    "    \"securigea varia\": \"securigera varia\",\n",
    "    \"melilotus officianalis\": \"melilotus officinalis\",\n",
    "    \"knautia maxima\": \"knautia dipsacifolia\",\n",
    "    \"hieracium aurantiacum\":\"pilosella aurantiaca\",\n",
    "    \"sysimbrium officinale\":\"sisymbrium officinale\",\n",
    "    \"geranium robertanium\":\"geranium robertianum\",\n",
    "    \"mycelis muralis\": \"lactuca muralis\",\n",
    "    \"calamintha-nepeta\":\"clinopodium nepeta\",\n",
    "    \"polygonum-persicaria\":\"persicaria maculosa\",\n",
    "    \"sorbus-aria\":\"aria edulis\",\n",
    "    \"taraxacum\": \"taraxacum officinale\"\n",
    "}\n",
    "\n",
    "# function to assign the correct value of the key is in the samples dictionary.\n",
    "\n",
    "def new_func(x,keys):\n",
    "    try:\n",
    "        data = keys[x]\n",
    "    except:\n",
    "        data = x\n",
    "    return data\n",
    "\n",
    "# apply the funtion to a copy of the surveys data set.\n",
    "\n",
    "samples[\"species2\"] = samples.sci.map(lambda x: new_func(x, replacedict))\n",
    "samples[\"species_slug\"] = samples.species2.map(lambda x: to_species_slug(x))\n",
    "\n",
    "# update the surveys dataset.\n",
    "\n",
    "data_and_keys.update({'surveys':samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sudden-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format dates in the sample dataset\n",
    "\n",
    "# function converts DD.MM.YYYY format to YYYY-MM-DD format, ignores if already in YYYY-MM-DD format\n",
    "\n",
    "def change_string(x):\n",
    "    try:\n",
    "        s_data = x.split('.')\n",
    "        data = s_data[::-1]\n",
    "        data = \"-\".join(data)\n",
    "    except:\n",
    "        print(\"no luck\")\n",
    "        data = x\n",
    "    \n",
    "    return data\n",
    "\n",
    "# applies the function to a column in the samples data frame\n",
    "\n",
    "samples['new_date'] = samples.date.map(lambda x: change_string(x))\n",
    "\n",
    "# function makes a timestamp out of the YYYY-MM-DD string.\n",
    "\n",
    "def make_timestamp(x):\n",
    "    try:        \n",
    "        data = dt.datetime.strptime(x, \"%Y-%m-%d\")        \n",
    "    except:        \n",
    "        data = 'no luck'\n",
    "    \n",
    "    return data\n",
    "\n",
    "# run the make_timestamp function and store the results in the samples dataframe.\n",
    "\n",
    "samples['stamp_date'] = samples.new_date.map(lambda x: make_timestamp(x))\n",
    "samples['date'] = samples.stamp_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dressed-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make place name slugs in the sample dataset\n",
    "\n",
    "#  change_place function turns \"place names 1\" into place-names-1\n",
    "def change_place(x):\n",
    "    data = x.split(\" \")\n",
    "    data = \"-\".join(data)\n",
    "    return data\n",
    "samples[\"place1\"] = samples.place.map(lambda x: change_place(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceramic-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up redundant and unused column names\n",
    "\n",
    "# rename the place_slug column\n",
    "\n",
    "samples.rename(columns= {'place1':'place_slug'}, inplace=True)\n",
    "\n",
    "# remove the unnecessary columns for this analysis\n",
    "\n",
    "samples.drop(['species2', 'stamp_date', 'place', 'sci', 'time', 'name'] , inplace=True, axis=1)\n",
    "\n",
    "samples['loc_date'] = list(zip(samples.place_slug, samples.date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cleared-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export samples to a .csv for later use\n",
    "samples.to_csv(F\"resources/preprocessed/hd_samples_2020.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bronze-answer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>species_slug</th>\n",
       "      <th>new_date</th>\n",
       "      <th>place_slug</th>\n",
       "      <th>loc_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>plantago-lanceolata</td>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>alleestrasse-1</td>\n",
       "      <td>(alleestrasse-1, 2020-09-02 00:00:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>centaurea-nigra</td>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>alleestrasse-1</td>\n",
       "      <td>(alleestrasse-1, 2020-09-02 00:00:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>plantago-media</td>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>alleestrasse-1</td>\n",
       "      <td>(alleestrasse-1, 2020-09-02 00:00:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>chenopodium-album</td>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>alleestrasse-1</td>\n",
       "      <td>(alleestrasse-1, 2020-09-02 00:00:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>centaurea-jacea</td>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>alleestrasse-1</td>\n",
       "      <td>(alleestrasse-1, 2020-09-02 00:00:00)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date         species_slug    new_date      place_slug  \\\n",
       "0 2020-09-02  plantago-lanceolata  2020-09-02  alleestrasse-1   \n",
       "1 2020-09-02      centaurea-nigra  2020-09-02  alleestrasse-1   \n",
       "2 2020-09-02       plantago-media  2020-09-02  alleestrasse-1   \n",
       "3 2020-09-02    chenopodium-album  2020-09-02  alleestrasse-1   \n",
       "4 2020-09-02      centaurea-jacea  2020-09-02  alleestrasse-1   \n",
       "\n",
       "                                loc_date  \n",
       "0  (alleestrasse-1, 2020-09-02 00:00:00)  \n",
       "1  (alleestrasse-1, 2020-09-02 00:00:00)  \n",
       "2  (alleestrasse-1, 2020-09-02 00:00:00)  \n",
       "3  (alleestrasse-1, 2020-09-02 00:00:00)  \n",
       "4  (alleestrasse-1, 2020-09-02 00:00:00)  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-overview",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
